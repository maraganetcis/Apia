import torch
import torch.nn as nn
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import json
import yaml
from model import WorldClassProgrammingAI
from dataset import CodeDataset
import os
from dotenv import load_dotenv

load_dotenv()

class WorldClassTrainer:
    def __init__(self, config_path="train_config.yaml"):
        self.config = self.load_config(config_path)
        self.model = None
        self.tokenizer = None
        self.train_dataset = None
        self.val_dataset = None
        
    def load_config(self, config_path):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config
    
    def setup_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •"""
        print("ğŸš€ ì„¸ê³„ 1ë“± AI ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        model_config = self.config['model']
        self.tokenizer = AutoTokenizer.from_pretrained(model_config['base_model'])
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            model_config['base_model'],
            load_in_8bit=model_config.get('load_in_8bit', True),
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def setup_training(self):
        """í•™ìŠµ ì„¤ì •"""
        print("ğŸ¯ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ í•™ìŠµ ì„¤ì • ì¤‘...")
        
        training_config = self.config['training']
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        self.train_dataset = CodeDataset(
            self.config['data']['train_file'], 
            self.tokenizer,
            max_length=4096
        )
        self.val_dataset = CodeDataset(
            self.config['data']['val_file'],
            self.tokenizer, 
            max_length=4096
        )
        
        # í•™ìŠµ ì¸ì ì„¤ì •
        self.training_args = TrainingArguments(
            output_dir="./world_class_checkpoints",
            overwrite_output_dir=True,
            num_train_epochs=training_config['num_train_epochs'],
            per_device_train_batch_size=training_config['per_device_train_batch_size'],
            per_device_eval_batch_size=training_config['per_device_eval_batch_size'],
            gradient_accumulation_steps=training_config['gradient_accumulation_steps'],
            warmup_steps=training_config['warmup_steps'],
            learning_rate=training_config['learning_rate'],
            fp16=training_config['fp16'],
            logging_steps=training_config['logging_steps'],
            save_steps=training_config['save_steps'],
            eval_steps=training_config['eval_steps'],
            save_total_limit=training_config['save_total_limit'],
            prediction_loss_only=True,
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            eval_accumulation_steps=1,
        )
        
        # ë°ì´í„° ì½œë ˆì´í„°
        self.data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        print("âœ… í•™ìŠµ ì„¤ì • ì™„ë£Œ!")
    
    def train(self):
        """ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ í•™ìŠµ ì‹¤í–‰"""
        print("ğŸ”¥ ì„¸ê³„ 1ë“± AI ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
        
        self.setup_model_and_tokenizer()
        self.setup_training()
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=self.training_args,
            data_collator=self.data_collator,
            train_dataset=self.train_dataset,
            eval_dataset=self.val_dataset,
        )
        
        # í•™ìŠµ ì‹œì‘
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model("./world_class_final_model")
        self.tokenizer.save_pretrained("./world_class_final_model")
        
        print("ğŸ‰ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ AI ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        return trainer

if __name__ == "__main__":
    trainer = WorldClassTrainer()
    trainer.train()
