# core_engine/model_loader.py
import torch
import os
from typing import Dict, Any, Optional
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import PeftModel, PeftConfig
import json

class ApiaModelLoader:
    """Apia ëª¨ë¸ ë¡œë“œ ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.loaded_models = {}
        self.model_configs = {}
        
    def load_model_with_quantization(self, 
                                   model_name: str,
                                   use_4bit: bool = True,
                                   use_8bit: bool = False,
                                   device_map: str = "auto") -> Dict[str, Any]:
        """ì–‘ìí™”ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ë¡œë“œ"""
        
        print(f"ğŸ”§ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        
        try:
            # ì–‘ìí™” ì„¤ì •
            quantization_config = None
            if use_4bit:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
            elif use_8bit:
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map=device_map,
                torch_dtype=torch.float16 if not use_4bit else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            model_info = {
                "model": model,
                "tokenizer": tokenizer,
                "name": model_name,
                "loaded_at": self._get_timestamp(),
                "quantization": "4bit" if use_4bit else "8bit" if use_8bit else "none"
            }
            
            self.loaded_models[model_name] = model_info
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            
            return model_info
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} - {e}")
            raise
    
    def load_peft_model(self, base_model_name: str, peft_model_path: str) -> Dict[str, Any]:
        """PEFT ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ”§ PEFT ëª¨ë¸ ë¡œë“œ: {peft_model_path}")
        
        try:
            # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
            base_model_info = self.load_model_with_quantization(base_model_name)
            base_model = base_model_info["model"]
            tokenizer = base_model_info["tokenizer"]
            
            # PEFT ëª¨ë¸ ë¡œë“œ
            model = PeftModel.from_pretrained(base_model, peft_model_path)
            
            model_info = {
                "model": model,
                "tokenizer": tokenizer,
                "name": f"{base_model_name}-peft",
                "peft_path": peft_model_path,
                "loaded_at": self._get_timestamp()
            }
            
            self.loaded_models[model_info["name"]] = model_info
            print(f"âœ… PEFT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {peft_model_path}")
            
            return model_info
            
        except Exception as e:
            print(f"âŒ PEFT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def unload_model(self, model_name: str):
        """ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ í•´ì œ"""
        if model_name in self.loaded_models:
            del self.loaded_models[model_name]
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            print(f"ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ: {model_name}")
    
    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        return self.loaded_models.get(model_name)
    
    def list_loaded_models(self) -> list:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return list(self.loaded_models.keys())
    
    def _get_timestamp(self) -> str:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def optimize_for_inference(self, model_info: Dict[str, Any]):
        """ì¶”ë¡  ìµœì í™”"""
        model = model_info["model"]
        
        # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
        model.eval()
        
        # ê·¸ë˜í”„ ìµœì í™”
        if hasattr(model, "config"):
            model.config.use_cache = True
        
        print("âš¡ ì¶”ë¡  ìµœì í™” ì™„ë£Œ")

# ì „ì—­ ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤
_model_loader = None

def get_model_loader():
    """ì „ì—­ ëª¨ë¸ ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ì–»ê¸°"""
    global _model_loader
    if _model_loader is None:
        _model_loader = ApiaModelLoader()
    return _model_loader

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    loader = ApiaModelLoader()
    print("Apia ëª¨ë¸ ë¡œë” ì¤€ë¹„ ì™„ë£Œ!")
